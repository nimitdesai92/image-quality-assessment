{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Image Quality Assessment This repository provides an implementation of an aesthetic and technical image quality model based on Google's research paper \"NIMA: Neural Image Assessment\" . You can find a quick introduction on their Research Blog . NIMA consists of two models that aim to predict the aesthetic and technical quality of images, respectively. The models are trained via transfer learning, where ImageNet pre-trained CNNs are used and fine-tuned for the classification task. For more information on how we used NIMA for our specifc problem, we did a write-up on two blog posts: NVIDIA Developer Blog: Deep Learning for Classifying Hotel Aesthetics Photos Medium: Using Deep Learning to automatically rank millions of hotel images The provided code allows to use any of the pre-trained models in Keras . We further provide Docker images for local CPU training and remote GPU training on AWS EC2, as well as pre-trained models on the AVA and TID2013 datasets. Read the full documentation at: https://idealo.github.io/image-quality-assessment/ . Image quality assessment is compatible with Python 3.6 and is distributed under the Apache 2.0 license. We welcome all kinds of contributions, especially new model architectures and/or hyperparameter combinations that improve the performance of the currently published models (see Contribute ). Trained models Predictions from aesthetic model Predictions from technical model We provide trained models, for both aesthetic and technical classifications, that use MobileNet as the base CNN. The models and their respective config files are stored under models/MobileNet . They achieve the following performance Model Dataset EMD LCC SRCC MobileNet aesthetic AVA 0.071 0.626 0.609 MobileNet technical TID2013 0.107 0.652 0.675 Getting started Install Docker Build docker image docker build -t nima-cpu . -f Dockerfile.cpu In order to train remotely on AWS EC2 Install Docker Machine Install AWS Command Line Interface Predict In order to run predictions on an image or batch of images you can run the prediction script Single image file . / predict \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images/42039.jpg All image files in a directory . / predict \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images Train locally on CPU Download dataset (see instructions under Datasets ) Run the local training script (e.g. for TID2013 dataset) . / train - local \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_technical.json \\ --samples-file $(pwd)/data/TID2013/tid_labels_train.json \\ --image-dir /path/to/image/dir/local This will start a training container from the Docker image nima-cpu and create a timestamp train job folder under train_jobs , where the trained model weights and logs will be stored. The --image-dir argument requires the path of the image directory on your local machine. In order to stop the last launched container run CONTAINER_ID = $ ( docker ps - l - q ) docker container stop $ CONTAINER_ID In order to stream logs from last launched container run CONTAINER_ID = $ ( docker ps - l - q ) docker logs $ CONTAINER_ID --follow Train remotely on AWS EC2 Configure your AWS CLI. Ensure that your account has limits for GPU instances and read/write access to the S3 bucket specified in config file [ link ] aws configure Launch EC2 instance with Docker Machine. Choose an Ubuntu AMI based on your region (https://cloud-images.ubuntu.com/locator/ec2/). For example, to launch a p2.xlarge EC2 instance named ec2-p2 run (NB: change region, VPC ID and AMI ID as per your setup) docker - machine create --driver amazonec2 \\ --amazonec2-region eu-west-1 \\ --amazonec2-ami ami-58d7e821 \\ --amazonec2-instance-type p2.xlarge \\ --amazonec2-vpc-id vpc-abc \\ ec2 - p2 ssh into EC2 instance docker - machine ssh ec2 - p2 Update NVIDIA drivers and install nvidia-docker (see this blog post for more details) # update NVIDIA drivers sudo add - apt - repository ppa : graphics - drivers / ppa - y sudo apt - get update sudo apt - get install - y nvidia - 375 nvidia - settings nvidia - modprobe # install nvidia - docker wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb && rm / tmp / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb Download dataset to EC2 instance (see instructions under Datasets ). We recommend to save the AMI with the downloaded data for future use. Run the remote EC2 training script (e.g. for AVA dataset) . / train - ec2 \\ --docker-machine ec2-p2 \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_aesthetic.json \\ --samples-file $(pwd)/data/AVA/ava_labels_train.json \\ --image-dir /path/to/image/dir/remote The training progress will be streamed to your terminal. After the training has finished, the train outputs (logs and best model weights) will be stored on S3 in a timestamped folder. The S3 output bucket can be specified in the config file . The --image-dir argument requires the path of the image directory on your remote instance. Contribute We welcome all kinds of contributions and will publish the performances from new models in the performance table under Trained models . For example, to train a new aesthetic NIMA model based on InceptionV3 ImageNet weights, you just have to change the base_model_name parameter in the config file models/MobileNet/config_mobilenet_aesthetic.json to \"InceptionV3\". You can also control all major hyperparameters in the config file, like learning rate, batch size, or dropout rate. See the Contribution guide for more details. Datasets This project uses two datasets to train the NIMA model: 1. AVA used for aesthetic ratings ( data ) 2. TID2013 used for technical ratings For training on AWS EC2 we recommend to build a custom AMI with the AVA images stored on it. This has proven much more viable than copying the entire dataset from S3 to the instance for each training job. Label files The train script requires JSON label files in the format [ { \"image_id\" : \"231893\" , \"label\" : [ 2 , 8 , 19 , 36 , 76 , 52 , 16 , 9 , 3 , 2 ] } , { \"image_id\" : \"746672\" , \"label\" : [ 1 , 2 , 7 , 20 , 38 , 52 , 20 , 11 , 1 , 3 ] } , ... ] The label for each image is the normalized or un-normalized frequency distribution of ratings 1-10. For the AVA dataset these frequency distributions are given in the raw data files. For the TID2013 dataset we inferred the normalized frequency distribution, i.e. probability distribution, by finding the maximum entropy distribution that satisfies the mean score. The code to generate the TID2013 labels can be found under data/TID2013/get_labels.py . For both datasets we provide train and test set label files stored under data / AVA / ava_labels_train . json data / AVA / ava_labels_test . json and data / TID2013 / tid2013_labels_train . json data / TID2013 / tid2013_labels_test . json For the AVA dataset we randomly assigned 90% of samples to the train set, and 10% to the test set, and throughout training a 5% validation set will be split from the training set to evaluate the training performance after each epoch. For the TID2013 dataset we split the train/test sets by reference images, to ensure that no reference image, and any of its distortions, enters both the train and test set. Serving NIMA with TensorFlow Serving TensorFlow versions of both the technical and aesthetic MobileNet models are provided, along with the script to generate them from the original Keras files, under the contrib/tf_serving directory. There is also an already configured TFS Dockerfile that you can use. To get predictions from the aesthetic or technical model: 1. Build the NIMA TFS Docker image docker build -t tfs_nima contrib/tf_serving 2. Run a NIMA TFS container with docker run -d --name tfs_nima -p 8500:8500 tfs_nima 3. Install python dependencies to run TF serving sample client virtualenv - p python3 contrib / tf_serving / venv_tfs_nima source contrib / tf_serving / venv_tfs_nima / bin / activate pip install - r contrib / tf_serving / requirements . txt Get predictions from aesthetic or technical model by running the sample client python - m contrib . tf_serving . tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_aesthetic python - m contrib . tf_serving . tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_technical Cite this work Please cite Image Quality Assessment in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2018imagequalityassessment , title = { Image Quality Assessment } , author = { Christopher Lennan and Hao Nguyen and Dat Tran } , year = { 2018 } , howpublished = {\\ url { https : // github . com / idealo / image - quality - assessment }} , } Maintainers Christopher Lennan, github: clennan Hao Nguyen, github: MrBanhBao Dat Tran, github: datitran Copyright See LICENSE for details.","title":"Home"},{"location":"#image-quality-assessment","text":"This repository provides an implementation of an aesthetic and technical image quality model based on Google's research paper \"NIMA: Neural Image Assessment\" . You can find a quick introduction on their Research Blog . NIMA consists of two models that aim to predict the aesthetic and technical quality of images, respectively. The models are trained via transfer learning, where ImageNet pre-trained CNNs are used and fine-tuned for the classification task. For more information on how we used NIMA for our specifc problem, we did a write-up on two blog posts: NVIDIA Developer Blog: Deep Learning for Classifying Hotel Aesthetics Photos Medium: Using Deep Learning to automatically rank millions of hotel images The provided code allows to use any of the pre-trained models in Keras . We further provide Docker images for local CPU training and remote GPU training on AWS EC2, as well as pre-trained models on the AVA and TID2013 datasets. Read the full documentation at: https://idealo.github.io/image-quality-assessment/ . Image quality assessment is compatible with Python 3.6 and is distributed under the Apache 2.0 license. We welcome all kinds of contributions, especially new model architectures and/or hyperparameter combinations that improve the performance of the currently published models (see Contribute ).","title":"Image Quality Assessment"},{"location":"#trained-models","text":"Predictions from aesthetic model Predictions from technical model We provide trained models, for both aesthetic and technical classifications, that use MobileNet as the base CNN. The models and their respective config files are stored under models/MobileNet . They achieve the following performance Model Dataset EMD LCC SRCC MobileNet aesthetic AVA 0.071 0.626 0.609 MobileNet technical TID2013 0.107 0.652 0.675","title":"Trained models"},{"location":"#getting-started","text":"Install Docker Build docker image docker build -t nima-cpu . -f Dockerfile.cpu In order to train remotely on AWS EC2 Install Docker Machine Install AWS Command Line Interface","title":"Getting started"},{"location":"#predict","text":"In order to run predictions on an image or batch of images you can run the prediction script Single image file . / predict \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images/42039.jpg All image files in a directory . / predict \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images","title":"Predict"},{"location":"#train-locally-on-cpu","text":"Download dataset (see instructions under Datasets ) Run the local training script (e.g. for TID2013 dataset) . / train - local \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_technical.json \\ --samples-file $(pwd)/data/TID2013/tid_labels_train.json \\ --image-dir /path/to/image/dir/local This will start a training container from the Docker image nima-cpu and create a timestamp train job folder under train_jobs , where the trained model weights and logs will be stored. The --image-dir argument requires the path of the image directory on your local machine. In order to stop the last launched container run CONTAINER_ID = $ ( docker ps - l - q ) docker container stop $ CONTAINER_ID In order to stream logs from last launched container run CONTAINER_ID = $ ( docker ps - l - q ) docker logs $ CONTAINER_ID --follow","title":"Train locally on CPU"},{"location":"#train-remotely-on-aws-ec2","text":"Configure your AWS CLI. Ensure that your account has limits for GPU instances and read/write access to the S3 bucket specified in config file [ link ] aws configure Launch EC2 instance with Docker Machine. Choose an Ubuntu AMI based on your region (https://cloud-images.ubuntu.com/locator/ec2/). For example, to launch a p2.xlarge EC2 instance named ec2-p2 run (NB: change region, VPC ID and AMI ID as per your setup) docker - machine create --driver amazonec2 \\ --amazonec2-region eu-west-1 \\ --amazonec2-ami ami-58d7e821 \\ --amazonec2-instance-type p2.xlarge \\ --amazonec2-vpc-id vpc-abc \\ ec2 - p2 ssh into EC2 instance docker - machine ssh ec2 - p2 Update NVIDIA drivers and install nvidia-docker (see this blog post for more details) # update NVIDIA drivers sudo add - apt - repository ppa : graphics - drivers / ppa - y sudo apt - get update sudo apt - get install - y nvidia - 375 nvidia - settings nvidia - modprobe # install nvidia - docker wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb && rm / tmp / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb Download dataset to EC2 instance (see instructions under Datasets ). We recommend to save the AMI with the downloaded data for future use. Run the remote EC2 training script (e.g. for AVA dataset) . / train - ec2 \\ --docker-machine ec2-p2 \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_aesthetic.json \\ --samples-file $(pwd)/data/AVA/ava_labels_train.json \\ --image-dir /path/to/image/dir/remote The training progress will be streamed to your terminal. After the training has finished, the train outputs (logs and best model weights) will be stored on S3 in a timestamped folder. The S3 output bucket can be specified in the config file . The --image-dir argument requires the path of the image directory on your remote instance.","title":"Train remotely on AWS EC2"},{"location":"#contribute","text":"We welcome all kinds of contributions and will publish the performances from new models in the performance table under Trained models . For example, to train a new aesthetic NIMA model based on InceptionV3 ImageNet weights, you just have to change the base_model_name parameter in the config file models/MobileNet/config_mobilenet_aesthetic.json to \"InceptionV3\". You can also control all major hyperparameters in the config file, like learning rate, batch size, or dropout rate. See the Contribution guide for more details.","title":"Contribute"},{"location":"#datasets","text":"This project uses two datasets to train the NIMA model: 1. AVA used for aesthetic ratings ( data ) 2. TID2013 used for technical ratings For training on AWS EC2 we recommend to build a custom AMI with the AVA images stored on it. This has proven much more viable than copying the entire dataset from S3 to the instance for each training job.","title":"Datasets"},{"location":"#label-files","text":"The train script requires JSON label files in the format [ { \"image_id\" : \"231893\" , \"label\" : [ 2 , 8 , 19 , 36 , 76 , 52 , 16 , 9 , 3 , 2 ] } , { \"image_id\" : \"746672\" , \"label\" : [ 1 , 2 , 7 , 20 , 38 , 52 , 20 , 11 , 1 , 3 ] } , ... ] The label for each image is the normalized or un-normalized frequency distribution of ratings 1-10. For the AVA dataset these frequency distributions are given in the raw data files. For the TID2013 dataset we inferred the normalized frequency distribution, i.e. probability distribution, by finding the maximum entropy distribution that satisfies the mean score. The code to generate the TID2013 labels can be found under data/TID2013/get_labels.py . For both datasets we provide train and test set label files stored under data / AVA / ava_labels_train . json data / AVA / ava_labels_test . json and data / TID2013 / tid2013_labels_train . json data / TID2013 / tid2013_labels_test . json For the AVA dataset we randomly assigned 90% of samples to the train set, and 10% to the test set, and throughout training a 5% validation set will be split from the training set to evaluate the training performance after each epoch. For the TID2013 dataset we split the train/test sets by reference images, to ensure that no reference image, and any of its distortions, enters both the train and test set.","title":"Label files"},{"location":"#serving-nima-with-tensorflow-serving","text":"TensorFlow versions of both the technical and aesthetic MobileNet models are provided, along with the script to generate them from the original Keras files, under the contrib/tf_serving directory. There is also an already configured TFS Dockerfile that you can use. To get predictions from the aesthetic or technical model: 1. Build the NIMA TFS Docker image docker build -t tfs_nima contrib/tf_serving 2. Run a NIMA TFS container with docker run -d --name tfs_nima -p 8500:8500 tfs_nima 3. Install python dependencies to run TF serving sample client virtualenv - p python3 contrib / tf_serving / venv_tfs_nima source contrib / tf_serving / venv_tfs_nima / bin / activate pip install - r contrib / tf_serving / requirements . txt Get predictions from aesthetic or technical model by running the sample client python - m contrib . tf_serving . tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_aesthetic python - m contrib . tf_serving . tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_technical","title":"Serving NIMA with TensorFlow Serving"},{"location":"#cite-this-work","text":"Please cite Image Quality Assessment in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2018imagequalityassessment , title = { Image Quality Assessment } , author = { Christopher Lennan and Hao Nguyen and Dat Tran } , year = { 2018 } , howpublished = {\\ url { https : // github . com / idealo / image - quality - assessment }} , }","title":"Cite this work"},{"location":"#maintainers","text":"Christopher Lennan, github: clennan Hao Nguyen, github: MrBanhBao Dat Tran, github: datitran","title":"Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"Copyright"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Quality Assessment repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use nosetests for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Quality Assessment repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use nosetests for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"Copyright 2018 idealo internet GmbH. All rights reserved. Apache License Version 2 . 0 , January 2004 http : // www . apache . org / licenses / TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work , attach the following boilerplate notice , with the fields enclosed by brackets \" [] \" replaced with your own identifying information . ( Don ' t include the brackets ! ) The text should be enclosed in the appropriate comment syntax for the file format . We also recommend that a file or class name and description of purpose be included on the same \" printed page \" as the copyright notice for easier identification within third - party archives . Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http : // www . apache . org / licenses / LICENSE - 2 . 0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"evaluater/predict/","text":"image_file_to_json def image_file_to_json ( img_path ) image_dir_to_json def image_dir_to_json ( img_dir , img_type ) predict def predict ( model , data_generator ) main def main ( base_model_name , weights_file , image_source , predictions_file , img_format )","title":"Predict"},{"location":"evaluater/predict/#image95file95to95json","text":"def image_file_to_json ( img_path )","title":"image_file_to_json"},{"location":"evaluater/predict/#image95dir95to95json","text":"def image_dir_to_json ( img_dir , img_type )","title":"image_dir_to_json"},{"location":"evaluater/predict/#predict","text":"def predict ( model , data_generator )","title":"predict"},{"location":"evaluater/predict/#main","text":"def main ( base_model_name , weights_file , image_source , predictions_file , img_format )","title":"main"},{"location":"handlers/config_loader/","text":"load_config def load_config ( config_file )","title":"Config Loader"},{"location":"handlers/config_loader/#load95config","text":"def load_config ( config_file )","title":"load_config"},{"location":"handlers/data_generator/","text":"class TrainDataGenerator inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator __init__ def __init__ ( samples , img_dir , batch_size , n_classes , basenet_preprocess , img_format , img_load_dims , img_crop_dims , shuffle ) __len__ def __len__ () __getitem__ def __getitem__ ( index ) on_epoch_end def on_epoch_end () __data_generator def __data_generator ( batch_samples ) class TestDataGenerator inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator __init__ def __init__ ( samples , img_dir , batch_size , n_classes , basenet_preprocess , img_format , img_load_dims ) __len__ def __len__ () __getitem__ def __getitem__ ( index ) on_epoch_end def on_epoch_end () __data_generator def __data_generator ( batch_samples )","title":"Data Generator"},{"location":"handlers/data_generator/#class-traindatagenerator","text":"inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator","title":"class TrainDataGenerator"},{"location":"handlers/data_generator/#9595init9595","text":"def __init__ ( samples , img_dir , batch_size , n_classes , basenet_preprocess , img_format , img_load_dims , img_crop_dims , shuffle )","title":"__init__"},{"location":"handlers/data_generator/#9595len9595","text":"def __len__ ()","title":"__len__"},{"location":"handlers/data_generator/#9595getitem9595","text":"def __getitem__ ( index )","title":"__getitem__"},{"location":"handlers/data_generator/#on95epoch95end","text":"def on_epoch_end ()","title":"on_epoch_end"},{"location":"handlers/data_generator/#9595data95generator","text":"def __data_generator ( batch_samples )","title":"__data_generator"},{"location":"handlers/data_generator/#class-testdatagenerator","text":"inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator","title":"class TestDataGenerator"},{"location":"handlers/data_generator/#9595init9595_1","text":"def __init__ ( samples , img_dir , batch_size , n_classes , basenet_preprocess , img_format , img_load_dims )","title":"__init__"},{"location":"handlers/data_generator/#9595len9595_1","text":"def __len__ ()","title":"__len__"},{"location":"handlers/data_generator/#9595getitem9595_1","text":"def __getitem__ ( index )","title":"__getitem__"},{"location":"handlers/data_generator/#on95epoch95end_1","text":"def on_epoch_end ()","title":"on_epoch_end"},{"location":"handlers/data_generator/#9595data95generator_1","text":"def __data_generator ( batch_samples )","title":"__data_generator"},{"location":"handlers/model_builder/","text":"class Nima __init__ def __init__ ( base_model_name , n_classes , learning_rate , dropout_rate , loss , decay , weights ) build def build () compile def compile () preprocessing_function def preprocessing_function ()","title":"Model Builder"},{"location":"handlers/model_builder/#class-nima","text":"","title":"class Nima"},{"location":"handlers/model_builder/#9595init9595","text":"def __init__ ( base_model_name , n_classes , learning_rate , dropout_rate , loss , decay , weights )","title":"__init__"},{"location":"handlers/model_builder/#build","text":"def build ()","title":"build"},{"location":"handlers/model_builder/#compile","text":"def compile ()","title":"compile"},{"location":"handlers/model_builder/#preprocessing95function","text":"def preprocessing_function ()","title":"preprocessing_function"},{"location":"handlers/samples_loader/","text":"load_samples def load_samples ( samples_file )","title":"Samples Loader"},{"location":"handlers/samples_loader/#load95samples","text":"def load_samples ( samples_file )","title":"load_samples"},{"location":"tests/test_augmentation_utils/","text":"class TestUtils test_random_crop def test_random_crop ( mock_np_random_randint ) test_random_flip def test_random_flip ( mock_np_random_randint ) test_normalize_label def test_normalize_label ()","title":"Test augmentation utils"},{"location":"tests/test_augmentation_utils/#class-testutils","text":"","title":"class TestUtils"},{"location":"tests/test_augmentation_utils/#test95random95crop","text":"def test_random_crop ( mock_np_random_randint )","title":"test_random_crop"},{"location":"tests/test_augmentation_utils/#test95random95flip","text":"def test_random_flip ( mock_np_random_randint )","title":"test_random_flip"},{"location":"tests/test_augmentation_utils/#test95normalize95label","text":"def test_normalize_label ()","title":"test_normalize_label"},{"location":"tests/test_data_generator/","text":"class TestTrainDataGenerator test_train_data_generator def test_train_data_generator () test_test_data_generator def test_test_data_generator ()","title":"Test data generator"},{"location":"tests/test_data_generator/#class-testtraindatagenerator","text":"","title":"class TestTrainDataGenerator"},{"location":"tests/test_data_generator/#test95train95data95generator","text":"def test_train_data_generator ()","title":"test_train_data_generator"},{"location":"tests/test_data_generator/#test95test95data95generator","text":"def test_test_data_generator ()","title":"test_test_data_generator"},{"location":"trainer/train/","text":"train def train ( base_model_name , n_classes , samples , image_dir , batch_size , epochs_train_dense , epochs_train_all , learning_rate_dense , learning_rate_all , dropout_rate , job_dir , img_format , existing_weights , multiprocessing_data_load , num_workers_data_load , decay_dense , decay_all , ** kwargs )","title":"Train"},{"location":"trainer/train/#train","text":"def train ( base_model_name , n_classes , samples , image_dir , batch_size , epochs_train_dense , epochs_train_all , learning_rate_dense , learning_rate_all , dropout_rate , job_dir , img_format , existing_weights , multiprocessing_data_load , num_workers_data_load , decay_dense , decay_all , ** kwargs )","title":"train"},{"location":"utils/keras_utils/","text":"class TensorBoardBatch __init__ def __init__ ( ** kwargs ) on_batch_end def on_batch_end ( batch , logs ) on_epoch_end def on_epoch_end ( epoch , logs )","title":"Keras Utils"},{"location":"utils/keras_utils/#class-tensorboardbatch","text":"","title":"class TensorBoardBatch"},{"location":"utils/keras_utils/#9595init9595","text":"def __init__ ( ** kwargs )","title":"__init__"},{"location":"utils/keras_utils/#on95batch95end","text":"def on_batch_end ( batch , logs )","title":"on_batch_end"},{"location":"utils/keras_utils/#on95epoch95end","text":"def on_epoch_end ( epoch , logs )","title":"on_epoch_end"},{"location":"utils/losses/","text":"earth_movers_distance def earth_movers_distance ( y_true , y_pred )","title":"Losses"},{"location":"utils/losses/#earth95movers95distance","text":"def earth_movers_distance ( y_true , y_pred )","title":"earth_movers_distance"},{"location":"utils/utils/","text":"load_json def load_json ( file_path ) save_json def save_json ( data , target_file ) random_crop def random_crop ( img , crop_dims ) random_horizontal_flip def random_horizontal_flip ( img ) load_image def load_image ( img_file , target_size ) normalize_labels def normalize_labels ( labels ) calc_mean_score def calc_mean_score ( score_dist ) ensure_dir_exists def ensure_dir_exists ( dir )","title":"Utils"},{"location":"utils/utils/#load95json","text":"def load_json ( file_path )","title":"load_json"},{"location":"utils/utils/#save95json","text":"def save_json ( data , target_file )","title":"save_json"},{"location":"utils/utils/#random95crop","text":"def random_crop ( img , crop_dims )","title":"random_crop"},{"location":"utils/utils/#random95horizontal95flip","text":"def random_horizontal_flip ( img )","title":"random_horizontal_flip"},{"location":"utils/utils/#load95image","text":"def load_image ( img_file , target_size )","title":"load_image"},{"location":"utils/utils/#normalize95labels","text":"def normalize_labels ( labels )","title":"normalize_labels"},{"location":"utils/utils/#calc95mean95score","text":"def calc_mean_score ( score_dist )","title":"calc_mean_score"},{"location":"utils/utils/#ensure95dir95exists","text":"def ensure_dir_exists ( dir )","title":"ensure_dir_exists"}]}